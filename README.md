CTCを使用した、連続音声認識の実験を行う。

１　トークンリストとラベルの作成
２　CTCの学習
３　CTCによる音声認識
４　音声認識結果の評価

【事前準備】
"data"や"ooprepare"のフォルダと同じ階層に"05ctc"というフォルダを作成する。
以降で紹介するソースコードは"05ctc"フォルダ内で実行することを想定している。

本ソースコードで必要となるlevenshteinのインストールは"pip"を使用せずに"conda install -c conda-forge python-levenshtein"を使用して行う。


【実行手順】
"01_get_token.py"⇨"encoder.py"⇨"my_model.py"⇨"02_train_ctc.py"⇨"03_decode_ctc.py"⇨"levenshtein.py"⇨"04_scoring.py"
CTCの音声認識のみを実行する場合は、"levenshtein.py", "04_scoring.py"は実行しなくてもよい。


【コード紹介】
"01_get_token.py"
学習データに出現するトークンを洗い出し、出現したトークンリストを作成する。また、各トークンの表記を数値に変換したラベルを作成するソースコード。
まず学習データのラベルファイルを開き、ラベルに出現したトークンをトークンリスト"token_list"に追加していく。
その後、作成したトークンリストを用いて、学習データ、開発データ、評価データのラベルを文字表記から数値表記に変換していく。
ただし、トークンリストは学習データから作成するため、学習データに存在しないトークンが開発データや評価データに出現した場合、それを認識することはできない。
また、トークンの定義に音素を用いる場合は、非音声トークンである"pau"を削除し、非音声部分はブランクトークンで対応させる


"encoder.py"
RNNと射影層の計算とサブサンプリングを"num_layers"の回数だけ行う部分までをエンコーダとして定義するソースコード。
Encoderクラスを作成し、オプションでRNNにLSTMかGRUのどちらを使用するかを選択できるようになっている。
LSTMを使用する場合は"torch.nn.LSTM"を、、GRUを使用する場合は、"torch.nn.GRU"をそれぞれ呼び出し、"num_layers"の数だけRNN層のリスト"rnn"に格納している。
RNN層の計算の前後で、"nn.utils.rnn.pack_padded_sequence"という関数と"nn.utils.rnn.pad_packed_sequence"という関数が実行されている。
Pytorchではミニバッチ内のデータサイズは統一されている必要があるため、入力音声系列はパディング処理を行なっているフレームの処理結果までRNNに記憶されてしまう。
これを避けるために、PytorchではPackedSequenceという特殊なデータ構造に変換する。PackedSequenceデータに変換する関数が、"nn.utils.rnn.pack_padded_sequence"
PackedSequenceに変換してからRNNを計算したあと、元のデータ構造に戻す関数が、"nn.utils.rnn.pad_packed_sequence"である。


"my_model.py"
エンコーダ出力に対してさらに出力層を計算するソースコード。
今ソースコードではCTCのモデルとしてMyCTCModelクラスを作成している。この中では、Encoderクラスを呼び出し、エンコーダの出力に対して出力層を計算している。またLeCunのパラメータ初期化も行なっている。


"02_train_ctc.py"
CTCを学習するソースコード。
このソースコードでは、特徴量として対数メルフィルタバンク出力を使用している。また、RNNは時間的依存関係を処理できるため、複数フレームの特徴量を結合するスプライシング処理は行っていない。

各種ハイパーパラメータの設定の後、トークンリストを読み込む。この時トークンリストの先頭(0番目)にブランクを付け加えている。つまりRNN出力の0番目のノードはブランクの確率になる。
次にMyCTCModelを読み込む。オプティマイザはAdaDelta("torch.optim.Adadelta")を使用している。
前向き後ろ向きアルゴリズムによるCTC損失関数の計算はPytorchの"torch.nn.CTCLoss"に実装されている。引数にブランクのインデクスが必要なため、"blank=0"を与えている。
DataLoaderからミニバッチを取り出した後、ミニバッチ内のデータはフレーム数が多い順にソートされている。
ミニバッチをモデルに入力すると、モデルの出力値に加えてその系列長も出力される。また出力系列長の情報はCTC損失関数計算じの引数として与えられる。これはパディング処理した部分を損失関数の計算に用いないようにするため。
勾配を計算する直前に、Gradient Clippingを行う。これは"torch.nn.utils.clip_grad_norm_"関数により実行される。学習が終わると"best_model.pt"に保存される。


"03_decode_ctc.py"
出力に対して連続する同一トークンとブランクを削除したものを音声認識結果とする。その処理を行うソースコード。
"cic_simple_decode"は入力されたフレーム単位の出力系列に対して、連続する同一トークンと文楽を削除し、最終的な音声認識結果を出力する関数。学習済みのモデルに評価データのミニバッチを入力し、
その出力"outputs"に対して、"_, hyp_per_frame = torch.max［outputs［idx］, 1］"とすることで、フレームごとに最も確率の高いトークンを"hyp_per_frame"に出力している。
その後、"hyp_per_frame"を"ctc_simple_decode"に入力することで、最終的な音声認識結果を得ている。


"levenshtein.py"
レーベンシュタイン距離と各誤りの数をカウントする関数のソースコード。


"04_scoring.py"
CTCの音声認識結果を評価するソースコード。
